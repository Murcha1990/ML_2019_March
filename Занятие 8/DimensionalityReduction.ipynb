{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Отбор признаков и снижение размерности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Понижение размерности можно использовать для следующих целей:\n",
    "\n",
    "* Сокращение ресурсоемкости алгоритмов\n",
    "* Ослабление влияния проклятия размерности и тем самым уменьшение переобучения\n",
    "* Переход к более информативным признакам\n",
    "\n",
    "Сейчас мы будем понижать размерность ориентируясь как раз на эти цели.\n",
    "Тогда этот процесс также можно называть и выделением признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отбор признаков\n",
    "\n",
    "Мы можем сократить количество исходных признаков несколькими способами. Первый - на основе **корреляции с целевой переменной**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "ds = load_boston()\n",
    "X_, y = ds.data, ds.target\n",
    "\n",
    "#добавим \"мешающий\" признак\n",
    "X = np.zeros((X_.shape[0],X_.shape[1]+1))\n",
    "X[:,:-1] = X_\n",
    "curr = np.random.randint(2, size=506)\n",
    "curr = np.array([elem if elem > 0 else elem-1 for elem in curr])\n",
    "X[:,-1] = X[:,0]*curr\n",
    "print(X.shape)\n",
    "\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "X = X[indices, :]\n",
    "y = y[indices]\n",
    "\n",
    "features_ind = np.arange(X.shape[1])\n",
    "corrs = np.abs([np.corrcoef(X[:, i], y)[0][1] for i in features_ind])\n",
    "importances_sort = np.argsort(corrs)\n",
    "plt.barh(features_ind, corrs[importances_sort])\n",
    "X = X[:, importances_sort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "features_counts = np.arange(1, X.shape[1] + 1)\n",
    "\n",
    "def scores_by_features_count(reg):\n",
    "    scores = []\n",
    "    for features_part in features_counts:\n",
    "        X_part = X[:, :features_part]\n",
    "        scores.append(cross_val_score(reg, X_part, y, cv=3).mean())\n",
    "    return scores\n",
    "    \n",
    "plt.figure()\n",
    "linreg_scores = scores_by_features_count(LinearRegression())\n",
    "plt.plot(features_counts, linreg_scores, label='LinearRegression')\n",
    "\n",
    "rf_scores = scores_by_features_count(RandomForestRegressor(n_estimators=100, max_depth=3))\n",
    "plt.plot(features_counts, rf_scores, label='RandomForest')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще один способ отбора признаков - с помощью метода **SelectKBest**. Метод оставляет k признаков с самыми большими значениями некоторой статистики, которую используем для отбора. Приведем пример, в качестве статистики использующий совместную информацию признаков. Для признаков X и Y она задается следующей формулой:\n",
    "\n",
    "$$I(X;Y)=\\sum _{y\\in Y}\\sum _{x\\in X}p(x,y)\\log {\\left({\\frac {p(x,y)}{p(x)\\,p(y)}}\\right)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_regression\n",
    "\n",
    "def scores_by_kbest_count(reg):\n",
    "    scores = []\n",
    "    for features_part in features_counts:\n",
    "        X_new = SelectKBest(mutual_info_regression, k=features_part).fit_transform(X, y)\n",
    "        scores.append(cross_val_score(reg, X_new, y, cv=3).mean())\n",
    "    return scores\n",
    "    \n",
    "#Plot results of SelectKBest\n",
    "#Your code is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Рекурсивный отбор признаков**\n",
    "\n",
    "Выбираем алгоритм (estimator), применяем его, и он в результате своей работы присваивает веса всем признакам. Затем откидываем наименее важные признаки и снова запускаем estimator и т.д., до тех пор, пока не останется заранее заданное число признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def scores_by_rfe_count(reg):\n",
    "    scores = []\n",
    "    for features_part in features_counts:\n",
    "        \n",
    "        est = RandomForestRegressor(n_estimators=10, max_depth=3)\n",
    "        #Try another estimators. Which one is the best?\n",
    "        \n",
    "        X_rfe = RFE(estimator=est, n_features_to_select=features_part, step=1).fit_transform(X, y)\n",
    "        scores.append(cross_val_score(reg, X_rfe, y, cv=3).mean())\n",
    "    return scores\n",
    "    \n",
    "#Plot results of RFE\n",
    "#Your code is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, что последние два метода при использовании RandomForestRegressor позволяют нам оставить довольно мало признаков, что существенно ускоряет нас."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод главных компонент (Principal Component Analysis, PCA)\n",
    "\n",
    "Выделение новых признаков путем их отбора часто дает плохие результаты, и\n",
    "в некоторых ситуациях такой подход практически бесполезен. Например, если\n",
    "мы работаем с изображениями, у которых признаками являются яркости пикселей,\n",
    "невозможно выбрать небольшой поднабор пикселей, который дает хорошую информацию о\n",
    "содержимом картинки. \n",
    "\n",
    "Поэтому признаки нужно как-то комбинировать. Рассмотрим метод главных компонент.\n",
    "\n",
    "Этот метод делает два важных упрощения задачи\n",
    "\n",
    "1. Игнорируется целевая переменная\n",
    "2. Строится линейная комбинация признаков\n",
    "\n",
    "П. 1 на первый взгляд кажется довольно странным, но на практике обычно не является\n",
    "таким уж плохим. Это связано с тем, что часто данные устроены так, что имеют какую-то\n",
    "внутреннюю структуру в пространстве меньшей размерности, которая никак не связана с\n",
    "целевой переменной. Поэтому и оптимальные признаки можно строить не глядя на ответ.\n",
    "\n",
    "П. 2 тоже сильно упрощает задачу, но далее мы научимся избавляться от него."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теория\n",
    "\n",
    "Обозначим $X$ - матрица объекты-признаки, с нулевым средним каждого признака,\n",
    "а $w$ - некоторый единичный вектор. Тогда\n",
    "$Xw$ задает величину проекций всех объектов на этот вектор. Далее ищется вектор,\n",
    "который дает наибольшую дисперсию полученных проекций (то есть наибольшую дисперсию\n",
    "вдоль этого направления):\n",
    "\n",
    "$$\n",
    "    \\max_{w: \\|w\\|=1} \\| Xw \\|^2 =  \\max_{w: \\|w\\|=1} w^T X^T X w\n",
    "$$\n",
    "\n",
    "Подходящий вектор тогда равен собственному вектору матрицы $X^T X$ с наибольшим собственным\n",
    "значением. После этого все пространство проецируется на ортогональное дополнение к вектору\n",
    "$w$ и процесс повторяется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA на плоскости\n",
    "\n",
    "Для начала посмотрим на метод PCA на плоскости для того, чтобы\n",
    "лучше понять, как он устроен.\n",
    "\n",
    "Создадим выборку из двух сильно зависящих признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1,11)\n",
    "y = 2 * x + np.random.randn(10)*2\n",
    "X = np.vstack((x,y))\n",
    "\n",
    "plt.scatter(X[0],X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Найдите m - вектор со средними значениями X,\n",
    "#и Xcentered = X-mean (хотим, чтобы среднее по каждой координате стало 0)\n",
    "#Your code is here\n",
    "Xcentered = ...\n",
    "m = ...\n",
    "\n",
    "print(\"Mean vector: \", m)\n",
    "print(Xcentered)\n",
    "\n",
    "plt.scatter(Xcentered[0],Xcentered[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисляем матрицу ковариаций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covmat = np.cov(Xcentered)\n",
    "print(covmat, \"\\n\")\n",
    "print(\"Variance of X: \", np.cov(Xcentered)[0,0])\n",
    "print(\"Variance of Y: \", np.cov(Xcentered)[1,1])\n",
    "print(\"Covariance X and Y: \", np.cov(Xcentered)[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Находим собственные векторы матрицы ковариаций и проецируем на самый большой вектор (с самым большим собственным значением) все точки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eignums, vecs = np.linalg.eig(covmat)\n",
    "v = vecs[:,np.argmax(eignums)]\n",
    "Xnew = np.dot(v,Xcentered)\n",
    "print(Xnew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Восстановление данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 9 #номер элемента случайной величины\n",
    "Xrestored = np.dot(Xnew[n],v) + m\n",
    "print('Restored: ', Xrestored)\n",
    "print('Original: ', X[:,n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = v[1]/v[0]\n",
    "\n",
    "x1 = np.arange(-6,7,0.1)\n",
    "y1 = [k*elem for elem in x1]\n",
    "plt.scatter(Xcentered[0],Xcentered[1])\n",
    "plt.scatter(x1,y1,color='red',s=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Доля объясненной дисперсии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100 * max(eignums)/sum(eignums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что нам выдаст PCA из sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Apply PCA with n_components=1 to transposed Xcentered\n",
    "#Your code is here\n",
    "pca = ...\n",
    "X_pca = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним полученный вектор главной компоненты с нашим ответом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(v)\n",
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Доля объясненного разброса по мнению встроенного PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set(style='white')\n",
    "from sklearn import datasets\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Загрузим наши ирисы\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Заведём красивую трёхмерную картинку\n",
    "fig = plt.figure(1, figsize=(6, 5))\n",
    "plt.clf()\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
    "\n",
    "plt.cla()\n",
    "\n",
    "for name, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:\n",
    "    ax.text3D(X[y == label, 0].mean(),\n",
    "              X[y == label, 1].mean() + 1.5,\n",
    "              X[y == label, 2].mean(), name,\n",
    "              horizontalalignment='center',\n",
    "              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))\n",
    "# Поменяем порядок цветов меток, чтобы они соответствовали правильному\n",
    "y_clr = np.choose(y, [1, 2, 0]).astype(np.float)\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y_clr, cmap=plt.cm.autumn)\n",
    "\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# Выделим из наших данных валидационную выборку\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, \n",
    "                                                    stratify=y, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# Для примера возьмём неглубокое дерево решений\n",
    "clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict_proba(X_test)\n",
    "print('Accuracy: {:.5f}'.format(accuracy_score(y_test, \n",
    "                                                preds.argmax(axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "%pylab inline\n",
    "\n",
    "#Apply PCA with n_components = 2. Don't forget to subtract mean from matrix X!\n",
    "#Your code is here\n",
    "X_pca = ...\n",
    "\n",
    "#Нарисуем получившиеся точки в нашем новом пространстве\n",
    "plt.plot(X_pca[y == 0, 0], X_pca[y == 0, 1], 'bo', label='Setosa')\n",
    "plt.plot(X_pca[y == 1, 0], X_pca[y == 1, 1], 'go', label='Versicolour')\n",
    "plt.plot(X_pca[y == 2, 0], X_pca[y == 2, 1], 'ro', label='Virginica')\n",
    "plt.legend(loc=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caclculate accuracy on reduced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, component in enumerate(pca.components_):\n",
    "    print(\"{} component: {}% of initial variance\".format(i + 1, \n",
    "          round(100 * pca.explained_variance_ratio_[i], 2)))\n",
    "    print(\" + \".join(\"%.3f x %s\" % (value, name)\n",
    "                     for value, name in zip(component,\n",
    "                                            iris.feature_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA (Linear Discriminant Analisys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При помощи метода линейного дискриминантного анализа выбирается проекция исходного пространства признаков на пространство признаков таким образом, чтобы минимизировать разброс точек внутри класса и максимизировать межклассовое расстояние в пространстве признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "Xsc = scaler.fit_transform(X)\n",
    "\n",
    "lda = LDA(n_components=2)\n",
    "X_lda = lda.fit(Xsc,y).transform(Xsc)\n",
    "\n",
    "# Нарисуйте получившиеся точки в нашем новом пространстве\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caclculate accuracy on new data after LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ссылки:\n",
    "    \n",
    "https://habr.com/ru/company/ods/blog/325654/\n",
    "\n",
    "https://habr.com/ru/post/304214/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
