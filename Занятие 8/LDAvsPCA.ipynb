{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA (Linear Discriminant Analysis). Сравнение LDA и PCA. Тематическое моделирование."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#запустить эту ячейку до начала занятия\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA,LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Главное отличие LDA от PCA: LDA - это алгоритм обучения с учителем, а PCA - нет.\n",
    "\n",
    "PCA находит направления с максимальной дисперсией и проектирует данные на эти направления.\n",
    "LDA создает новую ось таким образом, что при проецировании данных на эту ось объекты двух классов максимально разделяются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Iris = load_iris()\n",
    "\n",
    "data = Iris.data\n",
    "target = Iris.target\n",
    "target_names = Iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=np.concatenate((data,target.reshape(150,1)),axis=1),\\\n",
    "                  columns=['col_1','col_2','col_3','col_4','target'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['target'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_feature_reduced = pca.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_feature_reduced[:,0], X_feature_reduced[:,1], c=target)\n",
    "plt.title(\"PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=2)\n",
    "\n",
    "X_feature_reduced2 = lda.fit(df, target).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_feature_reduced2[:,0], X_feature_reduced2[:,1], c=target)\n",
    "plt.title('LDA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Наблюдение**\n",
    "\n",
    "Мы видим, что LDA проектирует данные на такую новую ось, что классы максимально разделены."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тематическое моделирование (topic modelling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тематическое моделирование - это присваивание темы (topic) каждому документу. Каждая тема представлена определенными словами.\n",
    "\n",
    "Рассмотрим пример:\n",
    "\n",
    "У нас есть два топика: топик 1 и топик 2. Топик1 представлен словами \"apple, banana, mange\",\n",
    "топик2 - словами \"tennis, cricket, hockey\". Можем предположить, что в топике1 речь идет о фруктах, а в топике2 - о спорте. Затем каждому новому документу мы присваиваем одну из этих тем (топик1 или топик2).\n",
    "\n",
    "Другой пример: предположим, у нас есть 6 документов\n",
    "\n",
    "apple banana\n",
    "apple orange\n",
    "banana orange\n",
    "tiger cat\n",
    "tiger dog\n",
    "cat dog\n",
    "\n",
    "Что будет происходить с тематическим моделированием, если мы захотим извлечь две темы (два топика) из этих документов?\n",
    "Мы получим два распределения: распределение тема-слово (topic-word) и распределение документ-тема (doc-topic).\n",
    "\n",
    "Идеальное распределение документ-слово в данном примере будет таким:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![How](df1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идеальное распределение документ-тема будет таким:\n",
    "\n",
    "![How](df2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, что у нас есть новый документ \"cat dog apple\", тогда его представление по темам должно быть следующим:\n",
    "\n",
    "Topic1: 0.33\n",
    "\n",
    "Topic2: 0.63\n",
    "\n",
    "LDA широко применяется в таких задачах. Его использование для тематического моделирования продемонстрировано ниже. \n",
    "\n",
    "Мы подаем на вход LDA число тем (topics), которые хотим выделить в корпусе. \n",
    "\n",
    "Но сначала необходимо векторизовать слова (будем использовать подход - мешок слов), поэтому взаимосвязь между словами в текстах при таком подходе исчезнет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() #For words Lemmatization\n",
    "stemmer = PorterStemmer()  #For stemming words\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TokenizeText(text):\n",
    "    ''' \n",
    "     Tokenizes text by removing various stopwords and lemmatizing them\n",
    "    '''\n",
    "    text=re.sub('[^A-Za-z0-9\\s]+', '', text)\n",
    "    word_list=word_tokenize(text)\n",
    "    word_list_final=[]\n",
    "    \n",
    "    for word in word_list:\n",
    "        if word not in stop_words:\n",
    "            word_list_final.append(lemmatizer.lemmatize(word))\n",
    "    return word_list_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gettopicwords(topics, cv, n_words=10):\n",
    "    '''\n",
    "        Print top n_words for each topic.\n",
    "        cv=Countvectorizer\n",
    "    '''\n",
    "    for i, topic in enumerate(topics):\n",
    "        top_words_array = np.array(cv.get_feature_names())[np.argsort(topic)[::-1][:n_words]]\n",
    "        print(\"For  topic {} it's top {} words are \".format(str(i),str(n_words)))\n",
    "             \n",
    "        combined_sentence=\"\"\n",
    "        for word in top_words_array:\n",
    "            combined_sentence+=word+\" \"\n",
    "        print(combined_sentence)\n",
    "#        print(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('million-headlines.zip',usecols=[1])\n",
    "df = df.iloc[:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data link:\n",
    "\n",
    "https://www.kaggle.com/therohk/million-headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "num_features = 100000\n",
    "# cv=CountVectorizer(min_df=0.01,max_df=0.97,tokenizer=TokenizeText,max_features=num_features)\n",
    "cv = CountVectorizer(tokenizer=TokenizeText, max_features=num_features)\n",
    "transformed_data = cv.fit_transform(df['headline_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "no_topics=10  ## We can change this, hyperparameter\n",
    "lda = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', \\\n",
    "                                learning_offset=50.,random_state=0,n_jobs=-1).fit(transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lda.components_ - это таблица тема-слово, она показывает, какими словами представлена каждая тема."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gettopicwords(lda.components_,cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Присваивание темы документу\n",
    "\n",
    "Можно заметить, что каждый документ содержит комбинацию тем. Посмотрим на темы первых десяти документов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df['headline_text'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for doc in docs:\n",
    "    data.append(lda.transform(cv.transform([doc])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['topic'+str(i) for i in range(1,11)]\n",
    "doc_topic_df = pd.DataFrame(columns=cols, data=np.array(data).reshape((10,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_df['major_topic'] = doc_topic_df.idxmax(axis=1)\n",
    "doc_topic_df['raw_doc'] = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы увидели, как LDA может быть использован для тематического моделирования. Такой подход также может быть применен для кластеризации документов, основанной на группировке по темам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ссылки\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/\n",
    "\n",
    "https://sebastianraschka.com/faq/docs/lda-vs-pca.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
