{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный бустинг\n",
    "\n",
    "Пусть дана обучающая выборка $X = \\{(x_i, y_i)\\}_{i=1}^l$ и выбран функционал качества $Q(a, X) = \\sum_{i=1}^l L(y_i, a(x_i)),$ который мы стремимся минимизировать. Градиентный бустинг строит композиции вида $a_N(x) = \\sum_{n=0}^N \\gamma_n b_n(x),$ где $b_n \\in \\mathcal{B}$ — базовые алгоритмы из некоторого семейства $\\mathcal{B}$.\n",
    "\n",
    "Композиция строится пошагово, на $n$-ом шаге к композиции добавляется алгоритм $b_n$ путём выполнения следующих действий:\n",
    " 1. Вычисление сдвигов текущей композиции по выборке $X$: $$s_i^{(n)} = - \\frac{\\partial L}{\\partial z} (y, z)\\Bigg|_{z = a_{n-1}(x_i)}$$\n",
    " 2. Обучение нового базового алгоритма на выборке $\\{(x_i, s_i)\\}_{i=1}^l$: $$b_n = \\arg\\min_{b \\in \\mathcal{B}} \\sum_{i=1}^l (b_n(x_i) - s_i)^2$$\n",
    " 3. Подбор коэффициента $\\gamma_n$ при новом базовом алгоритме:\n",
    " $$\\gamma_n = \\arg \\min_{\\gamma} \\sum_{i=1}^l L(y_i, a_{n-1}(x_i) + \\gamma b_n(x_i))$$\n",
    " \n",
    "Как обсуждалось ранее, при использовании градиентного бустинга можно понизить смещение моделей, но при этом разброс итоговой композиции останется таким же, как у отдельного базового алгоритма, или увеличится. Поэтому в качестве базовых алгоритмов удобно брать решающие деревья малой глубины, поскольку они обладают большим смещением и малым разбросом.\n",
    "\n",
    "\n",
    "## Градиентный бустинг и бэггинг\n",
    "\n",
    "Сравним, как ведут себя бустинг и бэггинг с ростом числа базовых алгоритмов.\n",
    "\n",
    "Напомним, что для построения композиции при помощи бэггинга из $X$ случайно и независимо выбираются подвыборки $\\tilde{X}_n, n = 1, \\dots, N.$ После этого на каждой из этих выборок обучается алгоритм из параметрического множества $\\mathcal{B},$ в результате чего получаем алгоритмы $b_n(x), n = 1, \\dots, N.$ Прогноз итоговой композиции строится следующим образом: \n",
    "\n",
    "$$a(x) = \\frac{1}{N} \\sum_{n=1}^N b_n(x)$$ (в случае регрессии),\n",
    "\n",
    "$$a(x) = \\arg \\max_{k = 1, \\dots, K} \\sum_{n=1}^N [b_n(x) = k]$$ (в случае классификации на K классов).\n",
    "\n",
    "В случае бэггинга все базовые алгоритмы настраиваются на различные выборки из одного и того же распределения на $\\mathbb{X} \\times \\mathbb{Y}$. При этом некоторые из них могут оказаться переобученными, однако усреднение позволяет ослабить этот эффект (объясняется тем, что для некоррелированных алгоритмов разброс композиции оказывается в $N$ раз меньше разброса отдельных алгоритмов, т.е. много деревьев с меньшей вероятностью настроятся на некоторый нетипичный объект по сравнению с одним деревом). Если $N$ достаточно велико, то последующие добавления новых алгоритмов уже не позволят улучшить качество модели.\n",
    "\n",
    "В случае же бустинга каждый алгоритм настраивается на ошибки всех предыдущих, это позволяет на каждом шаге настраиваться на исходное распределение все точнее и точнее. Однако при достаточно большом $N$ это является источником переобучения, поскольку последующие добавления новых алгоритмов будут продолжать настраиваться на обучающую выборку, уменьшая ошибку на ней, при этом уменьшая обобщающую способность итоговой композиции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- сгенерируем точки, разбитые на две группы: внутри каждой группы точки лежат в окрестности некоторой прямой\n",
    "- наша цель: понять, смогут ли бэггинг и градиентный бустинг понять эту зависимость (т.е. понять, что точки внутри каждйо группы лежат в окрестности некоторой прямой)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.linspace(0, 1, 100)\n",
    "X_test = np.linspace(0, 1, 1000)\n",
    "\n",
    "def target(x):\n",
    "    return x > 0.5\n",
    "\n",
    "Y_train = target(X_train) + np.random.randn(*X_train.shape) * 0.1\n",
    "\n",
    "plt.figure(figsize = (16, 9))\n",
    "plt.scatter(X_train, Y_train, s=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнём с BaggingRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, GradientBoostingRegressor\n",
    "from tqdm import tqdm\n",
    "\n",
    "reg = BaggingRegressor(DecisionTreeRegressor(max_depth=2), warm_start=True)\n",
    "plt.figure(figsize=(20, 30))\n",
    "sizes = [1, 2, 5, 20, 100, 500, 1000, 2000]\n",
    "for i, s in tqdm(enumerate(sizes)):\n",
    "    reg.n_estimators = s\n",
    "    reg.fit(X_train.reshape(-1, 1), Y_train)\n",
    "    plt.subplot(4, 2, i+1)\n",
    "    plt.xlim([0, 1])\n",
    "    plt.scatter(X_train, Y_train, s=30)\n",
    "    plt.plot(X_test, reg.predict(X_test.reshape(-1, 1)), c='green', linewidth=4)\n",
    "    plt.title('{} trees'.format(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что с некоторого момента итоговая функция перестает меняться с ростом количества деревьев.\n",
    "\n",
    "Теперь проделаем то же самое для градинентного бустинга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = GradientBoostingRegressor(max_depth=1, learning_rate=1, warm_start=True)\n",
    "plt.figure(figsize=(20, 30))\n",
    "sizes = [1, 2, 5, 20, 100, 500, 1000, 2000]\n",
    "for i, s in tqdm(enumerate(sizes)):\n",
    "    reg.n_estimators = s\n",
    "    reg.fit(X_train.reshape(-1, 1), Y_train)\n",
    "    plt.subplot(4, 2, i+1)\n",
    "    plt.xlim([0, 1])\n",
    "    plt.scatter(X_train, Y_train, s=30)\n",
    "    plt.plot(X_test, reg.predict(X_test.reshape(-1, 1)), c='green', linewidth=4)\n",
    "    plt.title('{} trees'.format(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный бустинг довольно быстро построил истинную зависимость, после чего начал настраиваться уже на конкретные объекты обучающей выборки, из-за чего сильно переобучился.\n",
    "\n",
    "\n",
    "Бороться с этой проблемой можно с помощью выбора очень простого базового алгоритма или\n",
    "же искусственным снижением веса новых алгоритмов при помощи шага $\\eta$:\n",
    "$$a_N(x) = \\sum_{n=0}^N \\eta \\gamma_N b_n(x).$$\n",
    "\n",
    "Такая поправка замедляет обучение по сравнению с бэггингом, но зато позволяет получить менее переобученный алгоритм. Тем не менее, важно понимать, что переобучение всё равно будет иметь место при обучении сколь угодно большого количества базовых алгоритмов для фиксированного $\\eta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = GradientBoostingRegressor(max_depth=1, learning_rate=0.1, warm_start=True)\n",
    "plt.figure(figsize=(20, 30))\n",
    "sizes = [1, 2, 5, 20, 100, 500, 1000, 2000]\n",
    "for i, s in tqdm(enumerate(sizes)):\n",
    "    reg.n_estimators = s\n",
    "    reg.fit(X_train.reshape(-1, 1), Y_train)\n",
    "    plt.subplot(4, 2, i+1)\n",
    "    plt.xlim([0, 1])\n",
    "    plt.scatter(X_train, Y_train, s=30)\n",
    "    plt.plot(X_test, reg.predict(X_test.reshape(-1, 1)), c='green', linewidth=4)\n",
    "    plt.title('{} trees'.format(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь проверим описанный выше эффект на реальных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ds = datasets.load_diabetes()\n",
    "print(ds.DESCR)\n",
    "X = ds.data\n",
    "Y = ds.target\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.5)\n",
    "\n",
    "MAX_ESTIMATORS = 250\n",
    "\n",
    "gbclf = GradientBoostingRegressor(warm_start=True)\n",
    "err_train_gb = []\n",
    "err_test_gb = []\n",
    "for i in range(1, MAX_ESTIMATORS+1):\n",
    "    gbclf.n_estimators = i\n",
    "    gbclf.fit(X_train, Y_train)\n",
    "    err_train_gb.append(1 - gbclf.score(X_train, Y_train))\n",
    "    err_test_gb.append(1 - gbclf.score(X_test, Y_test))\n",
    "\n",
    "gbclf = BaggingRegressor(warm_start=True)\n",
    "err_train_bag = []\n",
    "err_test_bag = []\n",
    "for i in range(1, MAX_ESTIMATORS+1):\n",
    "    gbclf.n_estimators = i\n",
    "    gbclf.fit(X_train, Y_train)\n",
    "    err_train_bag.append(1 - gbclf.score(X_train, Y_train))\n",
    "    err_test_bag.append(1 - gbclf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(err_train_gb, label='GB')\n",
    "plt.plot(err_train_bag, label='Bagging')\n",
    "plt.legend()\n",
    "plt.title('Train')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(err_test_gb, label='GB')\n",
    "plt.plot(err_test_bag, label='Bagging')\n",
    "plt.legend()\n",
    "plt.title('Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентный бустинг и случайные леса\n",
    "\n",
    "Сравним поведение двух методов построения композиции алгоримтов над деревьями на примере задачи [Kaggle: Predicting a Biological Response](https://www.kaggle.com/c/bioresponse):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "\n",
    "data = pd.read_csv('train.csv')\n",
    "X = data.iloc[:, 1:].values\n",
    "y = data.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=241)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gbm = GradientBoostingClassifier(n_estimators=250, learning_rate=0.2, verbose=True).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "for learning_rate in [1, 0.5, 0.3, 0.2, 0.1]:\n",
    "\n",
    "    gbm = GradientBoostingClassifier(n_estimators=250, learning_rate=learning_rate, random_state=241).fit(X_train, y_train)\n",
    "    \n",
    "    l = log_loss\n",
    "\n",
    "    test_deviance = np.zeros((gbm.n_estimators,), dtype=np.float64)\n",
    "    for i, y_pred in enumerate(gbm.staged_decision_function(X_test)):\n",
    "        y_pred = 1.0 / (1.0 + np.exp(-y_pred))\n",
    "        test_deviance[i] = l(y_test, y_pred)\n",
    "\n",
    "    train_deviance = np.zeros((gbm.n_estimators,), dtype=np.float64)\n",
    "    for i, y_pred in enumerate(gbm.staged_decision_function(X_train)):\n",
    "        y_pred = 1.0 / (1.0 + np.exp(-y_pred))\n",
    "        train_deviance[i] = l(y_train, y_pred)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(test_deviance, 'r', linewidth=2)\n",
    "    plt.plot(train_deviance, 'g', linewidth=2)\n",
    "    plt.legend(['test', 'train'])\n",
    "    plt.plot([0, train_deviance.shape[0]], [test_deviance.min(), test_deviance.min()], 'g--')\n",
    "    plt.plot([test_deviance.argmin()], [test_deviance.min()], 'v')\n",
    "    plt.title('GBM eta=%.1f, test logloss=%.3f, best_est=%d' % (learning_rate, test_deviance.min(), test_deviance.argmin()+1))\n",
    "    plt.xlabel('Number of trees')\n",
    "    plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого, лучшая композиция построена при $\\eta = 0.1$, включает 52 базовых алгоритма и достигает значения 0.527 на контрольной выборке. При этом случайный лес с таким же количеством базовых алгоритмов уступает градиентному бустингу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=52, random_state=241).fit(X_train, y_train)\n",
    "print ('Train RF log-loss =', log_loss(y_train, rf.predict_proba(X_train)))\n",
    "print ('Test RF log-loss = ', log_loss(y_test, rf.predict_proba(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим также, что при всём этом случайный лес, в отличие от градиентного бустинга, использует глубокие деревья, требующие вычислительных мощностей для их обучения.\n",
    "\n",
    "Посмотрим сколько базовых алгоритмов требуется случайному лесу для достижения такого же качества:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_estimators in range(10, 101, 10):\n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, n_jobs=4).fit(X_train, y_train)\n",
    "    print (n_estimators, 'trees: train log-loss =', log_loss(y_train, rf.predict_proba(X_train)), 'test log-loss =', log_loss(y_test, rf.predict_proba(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Свойства градиентного бустинга над решающими деревьями\n",
    "\n",
    "**Задача 1.** Пусть решается задача регрессии на одномерной выборке $X = \\{ (x_i, y_i)\\}_{i=1}^l,$ при этом истинная зависимость целевой переменной является линейной: $y(x) = ax + \\varepsilon, \\, \\varepsilon \\sim p(\\varepsilon) = \\mathcal{N} (0, \\sigma^2).$ Допустим, не зная этого, вы обучили на выборке линейную регрессию и решающее дерево с функционалом MSE, и вам известно, что модели не переобучились. После этого вы получили новые данные и построили на них прогнозы обеих моделей, и оказалось, что для решающего дерева значение функционала ошибки на новых данных оказалось радикально выше, чем для линейной регрессии. Чем это может быть вызвано?\n",
    "\n",
    "**Решение.**\n",
    "Поскольку истинная зависимость в данных является линейной, логично предположить, что линейная модель покажет лучшие результаты на подобной выборке. Опишем формально ситуацию, в которой у решающего дерева могут возникнуть серьезные проблемы с восстановлением истинной зависимости.\n",
    "\n",
    "Допустим, обучающая выборка была получена из отрезка $[0; 10],$ обучим соответствующие модели и построим прогнозы для этого отрезка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import rand, randn\n",
    "\n",
    "set_size = 100\n",
    "lin_coef = 3\n",
    "sigma = 1\n",
    "\n",
    "X_train = (rand(set_size) * 10).reshape(-1, 1)\n",
    "Y_train = X_train * 3 + sigma * randn(set_size).reshape(-1, 1)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.scatter(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, Y_train)\n",
    "tree = DecisionTreeRegressor()\n",
    "tree.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import rand, randn\n",
    "\n",
    "grid = np.arange(-1, 12, 0.1).reshape(-1, 1)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.scatter(X_train, Y_train)\n",
    "plt.plot(grid, lin_coef * grid, 'magenta')\n",
    "plt.plot(grid, lr.predict(grid), 'red',)\n",
    "plt.plot(grid, tree.predict(grid), 'green')\n",
    "plt.xlim([-1, 11])\n",
    "plt.legend(['Ground truth', 'Linear regression', 'Decision tree'], loc=0)\n",
    "print ('LR train MSE = ', mean_squared_error(Y_train, lr.predict(X_train)))\n",
    "print ('DT train MSE = ', mean_squared_error(Y_train, tree.predict(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, что новые данные были получены из другой области пространства ответов, например, из отрезка $[20; 30].$ В этом случае предсказания линейной регрессии окажутся гораздо ближе к правде, что отразится и на значении функционала ошибки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import rand, randn\n",
    "\n",
    "grid = np.arange(-1, 32, 0.1).reshape(-1, 1)\n",
    "\n",
    "X_test = (20 + rand(set_size) * 10).reshape(-1, 1)\n",
    "Y_test = X_test * 3 + sigma * randn(set_size).reshape(-1, 1)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.scatter(X_train, Y_train, c='blue')\n",
    "plt.scatter(X_test, Y_test, c='red')\n",
    "\n",
    "plt.plot(grid, lin_coef * grid, 'magenta')\n",
    "plt.plot(grid, lr.predict(grid), 'red',)\n",
    "plt.plot(grid, tree.predict(grid), 'green')\n",
    "plt.xlim([-1, 31])\n",
    "plt.legend(['Ground truth', 'Linear regression', 'Decision tree'], loc=0)\n",
    "print ('LR test MSE = ', mean_squared_error(Y_test, lr.predict(X_test)))\n",
    "print ('DT test MSE = ', mean_squared_error(Y_test, tree.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**: решающие деревья (а потому и композиции над ними, в т.ч. градиентный бустинг) непригодны для экстраполяции функций.\n",
    "\n",
    "В качестве решения этой проблемы в некоторых случаях можно использовать [нормализацию](http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-normalization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача 2.** Пусть решается задача регрессии для двумерной выборки $X = \\{ (x_i, y_i) \\}_{i=1}^l,$ при этом истинная зависимость целевой переменной $y(x) = y((x_1, x_2)) = sgn ((x_1 - 40)x_2).$ Предположим, вы обучили на выборке $X$ решающие деревья глубины 1, 2 и 3. Как соотносятся значения функционала $MSE$ для каждой из этих моделей? Почему? Какие зависимости позволяют улавливать глубокие деревья по сравнению с неглубокими?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = -50 + 100 * np.random.rand(100, 2)\n",
    "Y = np.sign((X[:, 0] - 40) * X[:, 1])\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, s=100, cmap='spring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grid(data):\n",
    "    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n",
    "    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n",
    "    return np.meshgrid(np.arange(x_min, x_max, 0.5),\n",
    "                         np.arange(y_min, y_max, 0.5))\n",
    "plt.figure(figsize=(20, 6))\n",
    "for i in range(3):\n",
    "    clf = DecisionTreeRegressor(random_state=42, max_depth = i + 1)\n",
    "\n",
    "    clf.fit(X, Y)\n",
    "    xx, yy = get_grid(X)\n",
    "    predicted = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    plt.subplot2grid((1, 3), (0, i))\n",
    "    plt.pcolormesh(xx, yy, predicted, cmap='spring')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y, s=30, cmap='spring')\n",
    "    plt.title('max_depth = ' + str(i + 1))\n",
    "    print('DT MSE (max_depth = ' + str(i+1) + ') = ', mean_squared_error(Y.reshape(-1, 1), clf.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно видеть, что с незначительным ростом глубины дерева ошибка на данных стремительно падает. Это связано с тем, что истинная зависимость целевой переменной учитывает взаимодействия признаков, а потому дерево глубины 1 в любом случае не может восстановить подобную зависимость, поскольку учитывается лишь один из признаков; дерево глубины 2 позволяет учесть взаимодействие пар признаков и т.д.\n",
    "\n",
    "**Вывод**: если вам известно, что в задаче необходимо учитывать взаимодействие $N$ признаков, корректным решением будет ограничивать глубину деревьев в градиентном бустинге числом, большим $N$. Кроме того, если вам из некоторых экспертных знаний известно, что на целевую переменную оказывает влияние конкретное взаимодействие различных признаков, следует добавить его явно в качестве признака, поскольку это может позволить повысить качество."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача 3.** Пусть дана некоторая выборка $X = \\{ (x_i, y_i)\\}_{i=1}^l.$ Как изменится решающее дерево, обученное на этой выборке, при масштабировании признаков?\n",
    "\n",
    "**Решение.** Рассмотрим некоторую вершину $m$ и множество $R_m$ объектов, попавших в неё. Заметим, что значение целевой переменной при масштабировании не изменится, а потому значение критерия информативности для каждого из возможных разбиений останется тем же самым (изменятся лишь значения порогов для каждого из признаков). В связи с этим в вершине $m$ в качестве оптимального будет выбрано то же разбиение, что и раньше. Поскольку это верно для любой вершины, построенное решающее дерево не изменится.\n",
    "\n",
    "Данные рассуждения также верны для любого монотонного преобразования признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Размер шага в градиентном бустинге\n",
    "\n",
    "Напомним, что в качестве меры для борьбы с переобучением в градиентном бустинге используют размер шага $\\eta$:\n",
    "$$a_N(x) = \\sum_{n=0}^N \\eta \\gamma_n b_n(x).$$\n",
    "\n",
    "Исследуем зависимость скорости сходимости градиентного бустинга в зависимости от размера шага и богатства семейства базовых алгоритмов $\\mathcal{B}$. На каждом шаге мы выбираем новый базовый алгоритм из семейства, обучая его на векторе сдвигов. Тем не менее, не всегда получается найти алгоритм, идеально приближающие посчитанный вектор сдвигов, поэтому иногда добавление нового базового алгоритма может \"увести\" нас в сторону.\n",
    "\n",
    "Рассмотрим обычный градиентный спуск и смоделируем описанную ситуацию, используя не честно посчитанный антиградиент, а его зашумленный вариант, и исследуем, за какое количество шагов (сколько базовых алгоритмов потребуется бустингу) мы сможем оказаться в окрестности оптимума в зависимости от длины шага:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(x, y):\n",
    "    return x**2 + 2 * y**2\n",
    "   \n",
    "def grad(x, y):\n",
    "    return np.array([2 * x, 4 * y])\n",
    "\n",
    "def make_data():\n",
    "    x = np.arange (-100, 100, 0.1)\n",
    "    y = np.arange (-100, 100, 0.1)\n",
    "    xgrid, ygrid = np.meshgrid(x, y)\n",
    "    zgrid = fun(xgrid, ygrid)\n",
    "#    zgrid = np.sin (xgrid) * np.sin (ygrid) / (xgrid * ygrid)\n",
    "    return xgrid, ygrid, zgrid\n",
    "\n",
    "\n",
    "def gradient_descent(numIterations, eta, noise):\n",
    "    ans = np.array([-70, 80])\n",
    "    x_points = [ans[0]]\n",
    "    y_points = [ans[1]]\n",
    "    for iter in range(0, numIterations):\n",
    "        ans = ans - eta * (grad(ans[0], ans[1]) + noise * np.random.randn(2))\n",
    "        x_points.append(ans[0])\n",
    "        y_points.append(ans[1])\n",
    "    return (x_points, y_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "X, Y, Z = make_data()\n",
    "\n",
    "for learning_rate in [0.5, 0.3, 0.1]:\n",
    "    x_points, y_points = gradient_descent(20, learning_rate, 10)\n",
    "    plt.figure(figsize = (7, 7))\n",
    "    CS = plt.contour(X, Y, Z)\n",
    "    plt.clabel(CS, inline=1, fontsize=10)\n",
    "    plt.plot(x_points, y_points, linestyle = '-', color = 'blue', alpha = 0.5)\n",
    "    plt.scatter(x_points[1:-1], y_points[1:-1], marker = '.', s=40, alpha = 0.5)\n",
    "    plt.scatter([x_points[0], x_points[-1]], [y_points[0], y_points[-1]], \n",
    "                marker = '*', s=100, color = 'red')\n",
    "    \n",
    "    plt.xlim([-100, 100])\n",
    "    plt.ylim([-100, 100])\n",
    "    plt.title('low noise, eta=%.1f' % learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если семейство алгоритмов $\\mathcal{B}$ более бедное, то направление движение на каждом шаге будет сильнее отличаться от антиградиента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "X, Y, Z = make_data()\n",
    "\n",
    "for learning_rate in [0.5, 0.3, 0.1]:\n",
    "    x_points, y_points = gradient_descent(20, learning_rate, 50)\n",
    "    plt.figure(figsize = (7, 7))\n",
    "    CS = plt.contour(X, Y, Z)\n",
    "    plt.clabel(CS, inline=1, fontsize=10)\n",
    "    plt.plot(x_points, y_points, linestyle = '-', color = 'blue', alpha = 0.5)\n",
    "    plt.scatter(x_points[1:-1], y_points[1:-1], marker = '.', s=40, alpha = 0.5)\n",
    "    plt.scatter([x_points[0], x_points[-1]], [y_points[0], y_points[-1]], \n",
    "                marker = '*', s=100, color = 'red')\n",
    "    \n",
    "    plt.xlim([-100, 100])\n",
    "    plt.ylim([-100, 100])\n",
    "    plt.title('high noise, eta=%.1f' % learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно видеть, что с уменьшением размера шага градиентному бустингу требуется больше базовых алгоритмов для достижения приемлемого качества композиции, однако при этом сходимость такого процесса надежнее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед вами задача классификации и её решение с помощью градиентного бустинга без настройки параметров. Настройте параметры алгоритма, чтобы добиться наибольшего качества.\n",
    "\n",
    "Попробуйте настраивать n_estimators, max_depth, learning_rate, другие параметры по желанию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "def plot_surface(X, y, clf):\n",
    "    h = 0.2\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "\n",
    "X, y = make_classification(n_samples=500, n_features=2, n_informative=2,\n",
    "                           n_redundant=0, n_repeated=0,\n",
    "                           n_classes=2, n_clusters_per_class=2,\n",
    "                           flip_y=0.05, class_sep=0.8, random_state=241)\n",
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(X, y)\n",
    "plot_surface(X, y, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=241)\n",
    "\n",
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нарисуйте график, показывающий качество (roc-auc) алгоритма в зависимости от числа деревьев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = list(np.arange(10,110,10))\n",
    "quals_train = []\n",
    "quals_test = []\n",
    "for n in n_trees:\n",
    "    #your code here\n",
    "    clf = ...\n",
    "    clf.fit(X_train,y_train)\n",
    "    preds_train = clf.predict_proba(X_train)[:,1]\n",
    "    preds_test = ...\n",
    "    quals_train.append(...)\n",
    "    quals_test.append(...)\n",
    "    \n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(n_trees, quals_train, marker='.', color='blue')\n",
    "plt.plot(n_trees, quals_test, marker='.', color='red')\n",
    "plt.xlabel('Number of trees')\n",
    "plt.ylabel('AUC-ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь можно посмотреть [визуализацию](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html) градиентного бустинга для решающих деревьев различной глубины для функций различного вида."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Задание\n",
    "\n",
    "Поработайте с датасетом Diabetes. Ваша задача - получить как можно более высокое качество на кросс-валидации (по метрике $R^2$).\n",
    "Вы можете использовать любые известные вам алгоритмы и методы обработки данных, но обязательно попробуйте DecisionTreeRegressor, RandomForestRegressor и GradientBoostingRegressor. Не забудьте подобрать параметры у алгоритмов.\n",
    "\n",
    "Для RandomForestRegressor и GradientBoostingRegressor с оптимальными параметрами постройте график зависимости качества алгоритмов от количества деревьев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "data = load_diabetes()\n",
    "print data.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = data.data\n",
    "y_full = data.target\n",
    "\n",
    "print X_full[:3]\n",
    "print X_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
